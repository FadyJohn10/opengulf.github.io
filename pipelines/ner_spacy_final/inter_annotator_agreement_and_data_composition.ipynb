{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inter_annotator_agreement and data composition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This pipeline extracts annotated entities and labels from training data for every annotator and preprocesses data for calculating inter-annotator agreement and calculates entity composition for each file."
      ],
      "metadata": {
        "id": "g3GfDdlyetZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hPfX9YBOD0EP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab00f39-1386-4224-c29b-a8b4bfead8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0ZLONgKBL0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30fe261-7def-4ae0-bffb-abbb8c9eec99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "#Download packages\n",
        "!python3 -m spacy download en\n",
        "!python3 -m spacy download en_core_web_sm\n",
        "\n",
        "#Import libraries\n",
        "import json \n",
        "import os\n",
        "import glob\n",
        "import spacy\n",
        "import random\n",
        "import en_core_web_sm\n",
        "from spacy import displacy\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute length and number of words for every file in training, testing and gold datasets"
      ],
      "metadata": {
        "id": "blYvBCu0yQUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the source folder\n",
        "import collections\n",
        "training_txt_path='/content/drive/MyDrive/Bushire-GT-compound/training_data_txt/'\n",
        "testing_txt_path='/content/drive/MyDrive/Bushire-GT-compound/inputs/'\n",
        "gold_txt_path='/content/drive/MyDrive/Bushire-GT-compound/gold_data/gold_inputs/'\n",
        "\n",
        "def compute_lengths(path,input_lengths):\n",
        "  new_path=path+\"*.txt\"\n",
        "  #Load all json files from the source folder and append them to the dictionary \n",
        "  for f in glob.glob(new_path):\n",
        "        try:\n",
        "          infile=open(f,encoding='utf8')\n",
        "          #extract name of the file\n",
        "          name=infile.name.split(\"/\")[-1]\n",
        "          print(name)\n",
        "          index=name.find(\".txt\")\n",
        "          name=name[:index]\n",
        "          #read file\n",
        "          text=infile.read()\n",
        "        except UnicodeDecodeError:\n",
        "          infile=open(f,encoding='windows-1252')\n",
        "          #extract name of the file\n",
        "          name=infile.name.split(\"/\")[-1]\n",
        "          index=name.find(\".txt\")\n",
        "          name=name[:index]\n",
        "          #read file\n",
        "          text=infile.read()\n",
        "        text=text.replace(\"\\n\", \" \")\n",
        "        text=text.replace(\"\\'\", \" \")\n",
        "        words = text.split()\n",
        "        input_lengths[name]=(len(text),len(words))\n",
        " \n",
        "#the dictionary consists of a tuple where first value = text length and second value=number of words\n",
        "gold_txt_lengths=collections.defaultdict(int)\n",
        "compute_lengths(gold_txt_path,gold_txt_lengths)\n",
        "\n",
        "training_txt_lengths=collections.defaultdict(int)\n",
        "compute_lengths(training_txt_path,training_txt_lengths)\n",
        "\n",
        "testing_txt_lengths=collections.defaultdict(int)\n",
        "compute_lengths(testing_txt_path,testing_txt_lengths)\n"
      ],
      "metadata": {
        "id": "0jiWYzR_yPMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb71d34-2ee1-4b6f-e664-6ab6a248facc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xby.txt\n",
            "xin.txt\n",
            "1825OCR_page15.txt\n",
            "iraq3.txt\n",
            "iraq1.txt\n",
            "iraq2.txt\n",
            "iraq4.txt\n",
            "iraq5.txt\n",
            "xaa.txt\n",
            "xah.txt\n",
            "xai.txt\n",
            "xcd.txt\n",
            "xcm.txt\n",
            "xeq.txt\n",
            "xla.txt\n",
            "xad.txt\n",
            "xac.txt\n",
            "xae.txt\n",
            "xaf.txt\n",
            "xbi.txt\n",
            "xbj.txt\n",
            "xab.txt\n",
            "xaa.txt\n",
            "iraq.txt\n",
            "xad.txt\n",
            "Book271,_pp_1-100.txt\n",
            "1825OCR_page23.txt\n",
            "1825OCR_page15.txt\n",
            "1825OCR_page12.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_txt_lengths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oxXYrhZ09xU",
        "outputId": "366a6ed0-e139-49e4-a65d-b6dfff616bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'iraq1': (3937, 646),\n",
              "             'iraq2': (3812, 651),\n",
              "             'iraq4': (4282, 705),\n",
              "             'iraq5': (4220, 737),\n",
              "             'xaa': (3505, 623),\n",
              "             'xad': (2628, 470),\n",
              "             'xah': (2351, 419),\n",
              "             'xai': (2530, 448),\n",
              "             'xcd': (3293, 609),\n",
              "             'xcm': (3859, 659),\n",
              "             'xeq': (1746, 316),\n",
              "             'xla': (2004, 346)})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_txt_lengths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W023pisZ4I4h",
        "outputId": "f81c858e-d9a4-42f0-a86a-f89703303947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'1825OCR_page12': (1171, 209),\n",
              "             '1825OCR_page15': (1424, 253),\n",
              "             '1825OCR_page23': (1594, 273),\n",
              "             'Book271,_pp_1-100': (69749, 12123),\n",
              "             'iraq': (272796, 46474),\n",
              "             'xaa': (2907, 515),\n",
              "             'xab': (1803, 315),\n",
              "             'xac': (3542, 628),\n",
              "             'xad': (2628, 470),\n",
              "             'xae': (2347, 415),\n",
              "             'xaf': (2664, 475),\n",
              "             'xbi': (1791, 318),\n",
              "             'xbj': (2187, 383)})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gold_txt_lengths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeLblq5eLZ31",
        "outputId": "8d27e87d-b8fa-4188-ee06-4203476c58b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'1825OCR_page15': (1424, 253),\n",
              "             'iraq3': (2517, 425),\n",
              "             'xby': (2332, 427),\n",
              "             'xin': (1458, 248)})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_lengths(dict_path,string):\n",
        "  total_length=0\n",
        "  total_symbols=0\n",
        "  for filename, tuple in dict_path.items():\n",
        "    total_length+=tuple[1]\n",
        "    total_symbols+=tuple[0]\n",
        "  print(string+\" data: number of words: \"+str(total_length)+\" and number of symbols: \"+str(total_symbols))\n",
        "\n",
        "\n",
        "print_lengths(gold_txt_lengths,\"gold\")\n",
        "print_lengths(training_txt_lengths,\"training\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5BWxt0f1B7p",
        "outputId": "551dd1c6-6ee1-4802-b810-95f1e4cc61a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gold data: number of words: 1353 and number of symbols: 7731\n",
            "training data: number of words: 6629 and number of symbols: 38167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load training data from every annotator and store them in a dictionary"
      ],
      "metadata": {
        "id": "yv2bdu0DBiwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the source folder\n",
        "source_path='/content/drive/MyDrive/Bushire-GT-compound/gold_data/gold_inputs/'\n",
        "#source_path='/content/drive/MyDrive/Bushire-GT-compound/training_data_json/'\n",
        "\n",
        "#Load all json files from the source folder and append them to the dictionary \n",
        "train_files = {} #-- name stores file name and value stores full json content\n",
        "train_annotations={} #-- name stores file name and value stores json content - only annotation section\n",
        "annotated_r={}\n",
        "annotated_d={}\n",
        "annotated_s={}\n",
        "for f in glob.glob(source_path+\"*.json\"):\n",
        "    with open(f) as infile:\n",
        "        name=infile.name.split(\"/\")[-1]\n",
        "        index=name.find(\".json\")\n",
        "        name=name[:index]\n",
        "        new_index=name.find(\"-\")\n",
        "        file_content=json.load(infile)\n",
        "        if new_index!=-1:\n",
        "          char=name[-1]\n",
        "          if char==\"r\":\n",
        "            annotated_r[name]=file_content['annotations']\n",
        "          elif char==\"s\":\n",
        "            annotated_s[name]=file_content['annotations']\n",
        "          elif char==\"d\":\n",
        "            annotated_d[name]=file_content['annotations']\n",
        "        train_files[name]=file_content['annotations']\n",
        "        #Store annotation section of each json file in a separate array\n",
        "        #if 'annotations' in train_files[name]:\n",
        "        train_annotations[name]=file_content['annotations']\n",
        "\n",
        "print(annotated_d)\n"
      ],
      "metadata": {
        "id": "-2c_yzd3Bhom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142c9e2a-cf4f-496a-da7e-30d95d256c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'xby-d': [['Mirza Mehedy Ally Khawn', {'entities': [[0, 23, 'PERSON']]}], ['1802', {'entities': [[0, 4, 'DATE']]}], ['Resident', {'entities': [[0, 8, 'TITLE']]}], ['Wm Bruce Asst.', {'entities': [[0, 13, 'PERSON']]}], ['Mehedi Alli Khawn', {'entities': [[0, 17, 'PERSON']]}], ['Resident at Bushier', {'entities': [[0, 8, 'TITLE'], [12, 19, 'GPE']]}], ['In the 2nd. Instant I received your letter of the 27th. Ultimo- The alert now returns', {'entities': [[7, 19, 'DATE'], [50, 63, 'DATE']]}], ['to Bushire on her way to the Presidency and I request that you will expedite her departure from', {'entities': [[3, 10, 'GPE'], [29, 39, 'ORG']]}], ['Bushire as much as may be possible', {'entities': [[0, 7, 'GPE']]}], ['remitting Me per Alert the Sum of Piasters 24000 / on the account of the Honble Company and', {'entities': [[34, 48, 'MONEY'], [73, 87, 'ORG']]}], ['I am to request that you will not in the present State of the Bushire Treasury think', {'entities': [[62, 78, 'ORG']]}], ['The Invoice of Wollens per Diamond which you', {'entities': [[15, 22, 'COMMODITY'], [27, 34, 'VESSEL']]}], ['Sales of the whole of the Consignments to Bushire of Broad Cloth and perpetuanoes', {'entities': [[42, 49, 'GPE'], [53, 64, 'COMMODITY']]}], ['per Ships Governor Duncan & Diamond', {'entities': [[4, 35, 'VESSEL']]}], ['Bussora 5th. May', {'entities': [[0, 7, 'GPE'], [8, 16, 'DATE']]}], ['1802 & received per Alert on', {'entities': [[0, 4, 'DATE'], [20, 25, 'VESSEL']]}], ['the 9th. May', {'entities': [[4, 12, 'DATE']]}], ['Samuel Manesty', {'entities': [[0, 14, 'PERSON']]}], ['Wm Bruce Asst.', {'entities': [[0, 13, 'PERSON']]}], ['The Honble the Governor & President', {'entities': [[4, 35, 'TITLE']]}], ['in Council & c & c', {'entities': [[3, 10, 'ORG']]}], ['Bombay', {'entities': [[0, 6, 'GPE']]}], ['I have the honor to forward the accompanying accounts of this Factory brought up to the', {'entities': [[62, 69, 'STRUCTURE']]}], ['30th. April', {'entities': [[0, 11, 'DATE']]}], ['On the 12th. Ultimo I received a letter from Samuel Manesty Esqr. Resident at Bussora', {'entities': [[7, 19, 'DATE'], [45, 65, 'PERSON'], [66, 74, 'TITLE'], [78, 85, 'GPE']]}], ['requesting me to forward by an early opportunity on English Vessels the Sum of 24000 Piasters', {'entities': [[52, 67, 'VESSEL'], [79, 93, 'MONEY']]}], ['( twenty four thousand) on the Honble Companys account provided the state of the Bushire treasury', {'entities': [[32, 47, 'ORG'], [82, 98, 'ORG']]}], ['As the ready Cash was not at this Factory to', {'entities': [[34, 41, 'STRUCTURE']]}], ['Company- the Draft I forwarded on the 5th Instant per Recovery Brig', {'entities': [[38, 49, 'DATE'], [54, 67, 'VESSEL']]}], ['Manestys request / as my instructions does not mention any thing respecting the Factorys', {'entities': [[0, 8, 'PERSON'], [80, 88, 'STRUCTURE']]}], ['The Atut Cruiser arrived here yesterday Evening from Bussora', {'entities': [[4, 16, 'VESSEL'], [30, 47, 'DATE'], [53, 60, 'GPE']]}], ['& sails from here tomorrow morning for the Presidency', {'entities': [[18, 34, 'DATE'], [43, 53, 'ORG']]}], ['PS Cherauik Ally Khawn Governor of', {'entities': [[0, 22, 'PERSON'], [23, 31, 'TITLE']]}], ['Shiraz has returned back from Court & has', {'entities': [[0, 6, 'GPE'], [30, 35, 'ORG']]}]], 'xin-d': [['by the express orders of Government._', {'entities': [[25, 37, 'ORG']]}], ['Bushire Resy.', {'entities': [[0, 12, 'GPE']]}], ['/Signed/ E.G. Stannus', {'entities': [[9, 21, 'PERSON']]}], ['3rd. Septr. 1826._', {'entities': [[0, 18, 'DATE']]}], ['Rest._', {'entities': [[0, 6, 'TITLE']]}], ['Political Department No.58.', {'entities': [[0, 20, 'ORG']]}], ['The Secretary to Government', {'entities': [[4, 27, 'TITLE']]}], ['=patches, by the H.C.C. Clive on the 1st. Instant Vizt.', {'entities': [[17, 29, 'VESSEL'], [37, 49, 'DATE']]}], ['1826 in the Political Department._', {'entities': [[0, 4, 'DATE'], [12, 34, 'ORG']]}], ['Bushire Resy.', {'entities': [[0, 12, 'GPE']]}], ['/Signed/ E.G. Stannus', {'entities': [[9, 21, 'PERSON']]}], ['3rd. Septr. 1826._', {'entities': [[0, 18, 'DATE']]}], ['Rest._', {'entities': [[0, 6, 'TITLE']]}], ['Political Departt._ No.59._', {'entities': [[0, 19, 'ORG']]}], ['The Secretary to Government ,', {'entities': [[4, 27, 'TITLE']]}], ['Despatches from Lt. Coll. Macdonald to the', {'entities': [[16, 35, 'PERSON']]}], ['address of the Chief Secretary, which were re=', {'entities': [[15, 30, 'TITLE']]}], ['ceived at Bushire on the 3rd. Instant._', {'entities': [[10, 17, 'GPE'], [25, 39, 'DATE']]}], ['/Signed/ E.G. Stannus', {'entities': [[9, 21, 'PERSON']]}], ['Bushire Resy.', {'entities': [[0, 12, 'GPE']]}], ['Rest._', {'entities': [[0, 6, 'TITLE']]}], ['3rd. Septr. 1826._', {'entities': [[0, 18, 'DATE']]}], ['Political Departt._ No.60.', {'entities': [[0, 19, 'ORG']]}], ['The Secretary to Government', {'entities': [[4, 27, 'TITLE']]}], [\"the Hon'ble the Governor, which has been re=\", {'entities': [[4, 24, 'TITLE']]}], ['=ceived this day from His Royal Highness', {'entities': [[22, 40, 'TITLE']]}], ['the Prince of Shiraz._ I likewise beg leave to', {'entities': [[4, 22, 'TITLE']]}], ['His Royal Highness and Zakee Khan, in reply', {'entities': [[0, 18, 'TITLE'], [23, 33, 'PERSON']]}], ['my Despatch dated the 16th. August.', {'entities': [[22, 35, 'DATE']]}], ['/Signed/ E.G. Stannus', {'entities': [[9, 21, 'PERSON']]}], ['Bushire Resy.', {'entities': [[0, 12, 'GPE']]}], ['Rest._', {'entities': [[0, 6, 'TITLE']]}], ['3rd. Septr. 1826._', {'entities': [[0, 18, 'DATE']]}]], 'iraq3-d': [[\"Population.—It is difficult to form even an approximate estimate of the population of 'Irāq, —an intricate and extensive country, parts of which are hardly ever visited by civilised travellers. In attempting to give some idea of the number of the inhabitants a distinction may first be drawn between what we may call the fixed and the nomadic elements; and among the former of these will be included not only the residents of towns and large villages but also a very great number of tribesmen who dwell in huts or even tents, yet devote themselves to agriculture and stock-raising and are generally found in the same localities, though not exactly at the same places.\", {'entities': [[86, 91, 'GPE']]}], ['', {'entities': []}], ['The following are three separate estimates of the fixed population in the Baghdād and Basrah Wilāyats: —', {'entities': [[74, 101, 'GPE']]}], ['', {'entities': []}], ['The first of these estimates (total 1202000) was deduced by the (German) Baghdād Railway Commission of 1900 from the Turkish official registers; the second (total 2865000) was specially supplied, it would appear, to the same body by the Wālis of Baghdād and Basrah6; the third (total 1480000) has been compiled, district by district, from information collected for the present Gazetteer, and a tabular analysis showing how it was reached will be found in the paragraph on civil administration below. The first two of these estimates no doubt include the populations of the Hasa Sanjāq and the Kuwait Principality in the figures for the Basrah Wilāyat, and to that extent they are excessive as estimates for \\'Irāq proper, which indeed they are not meant to be; but the first, on the other hand, is probably an underestimate for part of the country which it was intended to cover, for the Turkish official almanacs freely recognise the existence in some places of an \" unregistered \" population. Upon the whole the third estimate, referring as it does to \\'Irāq only and based on all the available information of every kind, is probably not very far from the truth.', {'entities': [[36, 43, 'QUANTITY'], [73, 99, 'ORG'], [103, 107, 'DATE'], [163, 170, 'QUANTITY'], [237, 265, 'TITLE'], [284, 291, 'QUANTITY'], [573, 584, 'GPE'], [593, 612, 'GPE'], [636, 650, 'GPE'], [707, 712, 'GPE']]}], ['', {'entities': []}], [\"The exact number of Bedouins in 'Irāq is impossible to calculate, and the great majority of those seen in the country belong in reality to Central or Eastern Arabia and have been included already in our estimates of the population of Jabal Shammar and Kuwait. The true nomads whose homes are in 'Irāq are far from numerous; 7 and they mostly belong to tribes of which the bulk is fixed or only semi-nomadic.\", {'entities': [[20, 28, 'TRIBE'], [32, 37, 'GPE'], [139, 164, 'LOC'], [234, 247, 'GPE'], [252, 259, 'GPE'], [295, 300, 'GPE']]}], ['', {'entities': []}], [\"On full consideration of all the facts the population of 'Irāq may be placed at 1500000 souls or slightly more, of whom only a very small proportion are wandering Bedouins.\", {'entities': [[57, 62, 'GPE'], [80, 93, 'QUANTITY'], [163, 171, 'TRIBE']]}], ['', {'entities': []}]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_r"
      ],
      "metadata": {
        "id": "Ow6v60GVET7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract entity and label (as a tuple) for every text file from every annotator and calculated Named Entity composition for every file"
      ],
      "metadata": {
        "id": "PFIL65ZLdjz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "total_labels=defaultdict(int)\n",
        "total_text_labels={}\n",
        "def populate(annotated_diction,total_labels,total_text_labels):\n",
        "  all_files_entities=defaultdict(list)\n",
        "  for filename in annotated_diction.keys():\n",
        "    orig_filename=filename\n",
        "    content=annotated_diction[filename]\n",
        "    index=filename.find(\"-\")\n",
        "    filename=filename[:index]\n",
        "    all_entities=[]\n",
        "    for content_file in content:\n",
        "      annotation=content_file[0]\n",
        "      #annotation=annotation.replace(\"\\'\", \"\")\n",
        "      #annotation=annotation.replace(\"'I\", \"I\")\n",
        "      entities=content_file[1]['entities']\n",
        "      if entities:\n",
        "        for ent in entities:\n",
        "          start=ent[0]\n",
        "          end=ent[1]\n",
        "          label=ent[2]\n",
        "          #increment number of labels\n",
        "          total_labels[label]+=1\n",
        "          #increment number of labels per textfile\n",
        "          if orig_filename not in total_text_labels:\n",
        "            total_text_labels[orig_filename]={}\n",
        "          if label in total_text_labels[orig_filename]:\n",
        "            total_text_labels[orig_filename][label]+=1\n",
        "          else:\n",
        "            total_text_labels[orig_filename][label]=1\n",
        "          entity_content=annotation[start:end]\n",
        "          tuple=(entity_content,label)\n",
        "          all_entities.append(tuple)\n",
        "    if all_entities:\n",
        "      all_files_entities[filename].append(all_entities)\n",
        "  return all_files_entities\n",
        "\n",
        "r_entities=populate(annotated_r,total_labels,total_text_labels)\n",
        "s_entities=populate(annotated_s,total_labels,total_text_labels)\n",
        "d_entities=populate(annotated_d,total_labels,total_text_labels)\n"
      ],
      "metadata": {
        "id": "ZJpdApTGYat6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_entities['xaa']"
      ],
      "metadata": {
        "id": "T9BJqcKBbMNF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04463f77-7237-4c4d-fafc-7b999d1c7dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_label_count=0\n",
        "for label,count in total_labels.items():\n",
        "  total_label_count+=count\n",
        "  print(label+\" \"+str(count))\n",
        "print(total_label_count)\n",
        "print(len(total_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKc5mJq-rK75",
        "outputId": "3b71c77c-89a9-4466-ef4c-1aa18029427b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERSON 22\n",
            "DATE 29\n",
            "TITLE 28\n",
            "GPE 33\n",
            "ORG 23\n",
            "VESSEL 15\n",
            "MONEY 4\n",
            "COMMODITY 5\n",
            "GPE_ORG 1\n",
            "STRUCTURE 6\n",
            "QUANTITY 5\n",
            "TRIBE 2\n",
            "LOC 1\n",
            "174\n",
            "13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_label_count=collections.defaultdict(int)\n",
        "for filename in total_text_labels:\n",
        "  labels=total_text_labels[filename]\n",
        "  total=0\n",
        "  for key in labels:\n",
        "    total+=labels[key]\n",
        "  total_label_count[filename]+=total\n",
        "print(total_label_count)\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh-nMPXttGF7",
        "outputId": "fc12a4ce-cb30-4bc4-e4bc-fef7e7cf535e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'xby-s': 62, 'xby-d': 54, 'xin-d': 37, 'iraq3-d': 21})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write every annotated file to the drive "
      ],
      "metadata": {
        "id": "TgsdIL5q2hO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Format: every line contains a Named Entity and label\n",
        "path=\"/content/drive/MyDrive/Bushire-GT-compound/gold_data/gold_annotator_output/\"\n",
        "def write_to_file(entity_dict,path,ext):\n",
        "  for filename in entity_dict:\n",
        "    out=entity_dict[filename][0]\n",
        "    filepath=path+filename+ext+\".txt\"\n",
        "    f = open((filepath), \"w\")\n",
        "    for tuple in out:\n",
        "      f.write(str(tuple[0])+\" \"+str(tuple[1]))\n",
        "      f.write(\"\\n\")\n",
        "    f.close()\n",
        "write_to_file(r_entities,path,\"-r\")\n",
        "write_to_file(d_entities,path,\"-d\")\n",
        "write_to_file(s_entities,path,\"-s\")"
      ],
      "metadata": {
        "id": "J7H716IhtOII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate annotator's output and model output"
      ],
      "metadata": {
        "id": "4VkifNqk5_Cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_output_path=\"/content/drive/MyDrive/Bushire-GT-compound/gold_data/gold_outputs/\"\n",
        "annotator_output_path=\"/content/drive/MyDrive/Bushire-GT-compound/gold_data/gold_annotator_output/\"\n",
        "for f in glob.glob(annotator_output_path+\"*.txt\"):\n",
        "      try:\n",
        "        infile=open(f,encoding='utf8')\n",
        "        #read file\n",
        "        text=infile.read()\n",
        "      except UnicodeDecodeError:\n",
        "        infile=open(f,encoding='windows-1252')\n",
        "        #read file\n",
        "        text=infile.read()\n",
        "      #extract name of the file\n",
        "      name=infile.name.split(\"/\")[-1]\n",
        "      index=name.find(\".txt\")\n",
        "      name=name[:index]\n",
        "      new_index=name.find(\"-\")\n",
        "      orig_name=name[:new_index]\n",
        "      text_arr=text.split(\"\\n\")\n",
        "      for pair in text_arr:\n",
        "        ent_arr=pair.split(\" \")\n",
        "        label=ent_arr[-1]\n",
        "        entity=\" \".join(ent_arr[0:len(ent_arr)-1])\n",
        "        print(entity)\n"
      ],
      "metadata": {
        "id": "tSwYUBKpQE8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9dd2b59-1cfa-4ca9-f0b1-9acb477280f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rupees\n",
            "Honble Company\n",
            "Rupees\n",
            "Secretary\n",
            "Honble Companys\n",
            "Resident\n",
            "Secretary-\n",
            "Bushire\n",
            "Bussora\n",
            "Country Boat\n",
            "Henry Moore\n",
            "George Skipp\n",
            "Mr. James Morley\n",
            "Resident\n",
            "Bushire\n",
            "Express\n",
            "Tyger\n",
            "Coja\n",
            "Stephan\n",
            "2400KY\n",
            "Dolphin Sohoonor\n",
            "Carim Caun\n",
            "Schiras\n",
            "Sohooner\n",
            "Honble Companys\n",
            "Bushire\n",
            "Residency-\n",
            "Honble Company\n",
            "Persian Dominions\n",
            "Bushire\n",
            "Bussora\n",
            "Tyger\n",
            "Rupees\n",
            "three Thousand/3000/\n",
            "Mr. Skipps\n",
            "Schirass/\n",
            "Bombay\n",
            "Bushire\n",
            "Mr Bouyears\n",
            "Bushire\n",
            "\n",
            "Government._\n",
            "Bushire Resy\n",
            "E.G. Stannus\n",
            "3rd. Septr. 1826._\n",
            "Rest._\n",
            "Political Department\n",
            "Secretary to Government\n",
            "H.C.C. Clive\n",
            "1st. Instant\n",
            "1826\n",
            "Political Department._\n",
            "Bushire Resy\n",
            "E.G. Stannus\n",
            "3rd. Septr. 1826._\n",
            "Rest._\n",
            "Political Departt._\n",
            "Secretary to Government\n",
            "Lt. Coll. Macdonald\n",
            "Chief Secretary\n",
            "Bushire\n",
            "3rd. Instant._\n",
            "E.G. Stannus\n",
            "Bushire Resy\n",
            "Rest._\n",
            "3rd. Septr. 1826._\n",
            "Political Departt._\n",
            "Secretary to Government\n",
            "Hon'ble the Governor\n",
            "His Royal Highness\n",
            "Prince of Shiraz._\n",
            "His Royal Highness\n",
            "Zakee Khan\n",
            "16th. August.\n",
            "E.G. Stannus\n",
            "Bushire Resy\n",
            "Rest._\n",
            "3rd. Septr. 1826._\n",
            "\n",
            "'Irāq\n",
            "Baghdād and Basrah Wilāyats\n",
            "1202000\n",
            "Baghdād Railway Commission\n",
            "1900\n",
            "2865000\n",
            "Wālis of Baghdād and Basrah6\n",
            "1480000\n",
            "Hasa Sanjāq\n",
            "Kuwait Principality\n",
            "Basrah Wilāyat\n",
            "'Irāq\n",
            "Bedouins\n",
            "'Irāq\n",
            "Central or Eastern Arabia\n",
            "Jabal Shammar\n",
            "Kuwait.\n",
            "'Irāq\n",
            "'Irāq\n",
            "1500000 souls\n",
            "Bedouins\n",
            "\n",
            "Dolphin Schooner-\n",
            "400 Rupees \n",
            "xpress Boat,\n",
            "olphin \n",
            "ushire \n",
            "otsey \n",
            "hire wi\n",
            "o 9 per Cont to\n",
            "the gre\n",
            " or any othe\n",
            "esent good \n",
            "during his re\n",
            "e is we \n",
            "edience to \n",
            " Countr\n",
            "w accomodat\n",
            "apassage\n",
            "l suppl\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Credits: https://towardsdatascience.com/inter-annotator-agreement-2f46c6d37bf3 \n",
        "def cohen_kappa(ann1, ann2):\n",
        "    \"\"\"Computes Cohen kappa for pair-wise annotators.\n",
        "    :param ann1: annotations provided by first annotator\n",
        "    :type ann1: list\n",
        "    :param ann2: annotations provided by second annotator\n",
        "    :type ann2: list\n",
        "    :rtype: float\n",
        "    :return: Cohen kappa statistic\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    for an1, an2 in zip(ann1, ann2):\n",
        "        if an1 == an2:\n",
        "            count += 1\n",
        "    A = count / len(ann1)  # observed agreement A (Po)\n",
        "\n",
        "    uniq = set(ann1 + ann2)\n",
        "    E = 0  # expected agreement E (Pe)\n",
        "    for item in uniq:\n",
        "        cnt1 = ann1.count(item)\n",
        "        cnt2 = ann2.count(item)\n",
        "        count = ((cnt1 / len(ann1)) * (cnt2 / len(ann2)))\n",
        "        E += count\n",
        "\n",
        "    return round((A - E) / (1 - E), 4)\n",
        "\n"
      ],
      "metadata": {
        "id": "aBwYPvg5QIZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#Credits: https://www.datacamp.com/community/tutorials/fuzzy-string-python\n",
        "def levenshtein_ratio_and_distance(s, t, ratio_calc = False):\n",
        "    \"\"\" levenshtein_ratio_and_distance:\n",
        "        Calculates levenshtein distance between two strings.\n",
        "        If ratio_calc = True, the function computes the\n",
        "        levenshtein distance ratio of similarity between two strings\n",
        "        For all i and j, distance[i,j] will contain the Levenshtein\n",
        "        distance between the first i characters of s and the\n",
        "        first j characters of t\n",
        "    \"\"\"\n",
        "    # Initialize matrix of zeros\n",
        "    rows = len(s)+1\n",
        "    cols = len(t)+1\n",
        "    distance = np.zeros((rows,cols),dtype = int)\n",
        "\n",
        "    # Populate matrix of zeros with the indeces of each character of both strings\n",
        "    for i in range(1, rows):\n",
        "        for k in range(1,cols):\n",
        "            distance[i][0] = i\n",
        "            distance[0][k] = k\n",
        "\n",
        "    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    \n",
        "    for col in range(1, cols):\n",
        "        for row in range(1, rows):\n",
        "            if s[row-1] == t[col-1]:\n",
        "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
        "            else:\n",
        "                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio\n",
        "                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.\n",
        "                if ratio_calc == True:\n",
        "                    cost = 2\n",
        "                else:\n",
        "                    cost = 1\n",
        "            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions\n",
        "                                 distance[row][col-1] + 1,          # Cost of insertions\n",
        "                                 distance[row-1][col-1] + cost)     # Cost of substitutions\n",
        "    if ratio_calc == True:\n",
        "        # Computation of the Levenshtein Distance Ratio\n",
        "        Ratio = ((len(s)+len(t)) - distance[row][col]) / (len(s)+len(t))\n",
        "        return Ratio\n",
        "    else:\n",
        "        # print(distance) # Uncomment if you want to see the matrix showing how the algorithm computes the cost of deletions,\n",
        "        # insertions and/or substitutions\n",
        "        # This is the minimum number of edits needed to convert string a to string b\n",
        "        return (\"The strings are {} edits away\".format(distance[row][col]),distance[row][col])"
      ],
      "metadata": {
        "id": "E8wHu1-aWXUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find approximately matching strings to solve the incorrect tokenization problem"
      ],
      "metadata": {
        "id": "ZQGA38sMbHKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we find at least 40% matching strings using the levenstein ratio formula\n",
        "def matching_named_entities(annot1,annot2,matching,non_matching,annotations1,annotations2):\n",
        "  for filename in annot1:\n",
        "    if filename in annot2:\n",
        "      #print(filename)\n",
        "      content1=annot1[filename][0]\n",
        "      content2=annot2[filename][0]\n",
        "      #print(len(content1))\n",
        "      for tuple1 in content1:        \n",
        "        text1=tuple1[0]\n",
        "        label1=tuple1[1]\n",
        "        for tuple2 in content2:\n",
        "          text2=tuple2[0]\n",
        "          label2=tuple2[1]\n",
        "          dist=levenshtein_ratio_and_distance(text1,text2)[1]\n",
        "          len1=len(text1)\n",
        "          len2=len(text2)\n",
        "          if dist<(0.7*min(len1,len2)):\n",
        "            annotations1.add(text1)\n",
        "            annotations2.add(text2)\n",
        "            if label1==label2:\n",
        "              matching[label1].add((text1,text2))\n",
        "            else:\n",
        "              if (label1,label2) in non_matching:\n",
        "                non_matching[(label1,label2)].add((text1,text2))\n",
        "              elif (label2,label1) in non_matching:\n",
        "                non_matching[(label2,label1)].add((text1,text2))\n",
        "              else:\n",
        "                non_matching[(label1,label2)].add((text1,text2))\n",
        "          \n",
        "annotations1=set()\n",
        "annotations2=set()\n",
        "matching=defaultdict(set)\n",
        "non_matching=defaultdict(set)      \n",
        "matching_named_entities(r_entities,s_entities,matching,non_matching,annotations1,annotations2)\n"
      ],
      "metadata": {
        "id": "lXaUqpduLDX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matching"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRFX05mLdJN5",
        "outputId": "6e844411-cfb3-4d0a-a2dd-a323dff1bb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(set,\n",
              "            {'GPE': {('Bahreen', 'hreen, '),\n",
              "              ('Bombay', 'Bombay'),\n",
              "              ('Bombay', 'mbay f'),\n",
              "              ('Bombay', 'mbay; '),\n",
              "              ('Bombay', 'ombay;'),\n",
              "              ('Bushire', 'Bushire'),\n",
              "              ('Bushire', 'Bushire-'),\n",
              "              ('Bushire', 'Bushire-On'),\n",
              "              ('Bushire', 'Bushire/'),\n",
              "              ('Bushire', 'Bussora'),\n",
              "              ('Bushire', 'Bussora/'),\n",
              "              ('Bushire', 'shire b'),\n",
              "              ('Bushire', 'shire i'),\n",
              "              ('Bushire', 'shire s'),\n",
              "              ('Bushire', 'shire, '),\n",
              "              ('Bushire-', 'Bushire'),\n",
              "              ('Bushire-', 'Bushire-'),\n",
              "              ('Bushire-', 'Bushire-On'),\n",
              "              ('Bushire-', 'Bushire/'),\n",
              "              ('Bushire-', 'Bussora/'),\n",
              "              ('Bushire-On', 'Bushire'),\n",
              "              ('Bushire-On', 'Bushire-'),\n",
              "              ('Bushire-On', 'Bushire-On'),\n",
              "              ('Bushire-On', 'Bushire/'),\n",
              "              ('Bushire/', 'Bushire'),\n",
              "              ('Bushire/', 'Bushire-'),\n",
              "              ('Bushire/', 'Bushire-On'),\n",
              "              ('Bushire/', 'Bushire/'),\n",
              "              ('Bushire/', 'Bussora/'),\n",
              "              ('Bussora', 'Bushire'),\n",
              "              ('Bussora', 'Bussora'),\n",
              "              ('Bussora', 'ssora a'),\n",
              "              ('Bussora/', 'Bushire'),\n",
              "              ('Bussora/', 'Bushire-'),\n",
              "              ('Bussora/', 'Bushire/'),\n",
              "              ('Bussora/', 'Bussora/'),\n",
              "              ('Muscat', 'Muscat'),\n",
              "              ('Scherass', 'Scherass'),\n",
              "              ('Scherass', 'Schyrass'),\n",
              "              ('Schiras', 'shire s'),\n",
              "              ('Schirass', 'Schirass'),\n",
              "              ('Schirass', 'Schyrass_'),\n",
              "              ('Schyrass', 'Scherass'),\n",
              "              ('Schyrass', 'Schyrass'),\n",
              "              ('Schyrass_', 'Schirass'),\n",
              "              ('Schyrass_', 'Schyrass_')},\n",
              "             'ORG': {('Agency', 'Agency'),\n",
              "              ('Council', 'Council'),\n",
              "              ('Embassy', 'Embassy'),\n",
              "              ('Guard', 'Guard'),\n",
              "              ('Presidency', 'Presidency'),\n",
              "              ('Residency', 'Residency'),\n",
              "              ('Residency', 'Residency Guard')},\n",
              "             'PERSON': {('=hangeer Khan', 'Jehangeer Khan'),\n",
              "              ('Benjamin Jervis', 'Benjamin Jervis'),\n",
              "              ('Captain Stennits', 'Captain Stennits'),\n",
              "              ('Captn. Betham', 'Captn. Betham'),\n",
              "              ('Carim Caun', 'Carim Caun'),\n",
              "              ('Daniel Draper', 'Daniel Draper'),\n",
              "              ('E.G. Stannus', 'E.G. Stannus'),\n",
              "              ('Henry Moore', 'Henry Moore'),\n",
              "              ('Henry Moore', 'James Moorley'),\n",
              "              ('Hossein', 'Hossein'),\n",
              "              ('Hossein', 'Hossen'),\n",
              "              ('Hossein Mahomed', 'Hossein Mahomed'),\n",
              "              ('Hossein Mahomed', 'Hossen Mahomed'),\n",
              "              ('Hossein Mahomed', 'Hossin Mohumed'),\n",
              "              ('Hossen', 'Hossein'),\n",
              "              ('Hossen', 'Hossen'),\n",
              "              ('Hossen Mahomed', 'Hossein Mahomed'),\n",
              "              ('Hossen Mahomed', 'Hossen Mahomed'),\n",
              "              ('Hossen Mahomed', 'Hossin Mohumed'),\n",
              "              ('Hossin Mohumed', 'Hossein Mahomed'),\n",
              "              ('Hossin Mohumed', 'Hossen Mahomed'),\n",
              "              ('Hossin Mohumed', 'Hossin Mohumed'),\n",
              "              ('Hossin Mohumed', 'Shaik Moideen'),\n",
              "              ('James Moorley', 'Henry Moore'),\n",
              "              ('James Moorley', 'James Moorley'),\n",
              "              ('James Moorley', 'James Morley'),\n",
              "              ('James Morley', 'James Moorley'),\n",
              "              ('James Morley', 'James Morley'),\n",
              "              ('James Ryley', 'James Ryley'),\n",
              "              ('James Ryley', 'Mr James Moorley'),\n",
              "              ('James Ryley', 'Mr. James Morley'),\n",
              "              ('Jehangeer Khan', 'Jehangeer Khan'),\n",
              "              ('Mr George Green', 'Mr George Green'),\n",
              "              ('Mr James Moorley', 'James Ryley'),\n",
              "              ('Mr James Moorley', 'Mr James Moorley'),\n",
              "              ('Mr James Moorley', 'Mr. James Morley'),\n",
              "              ('Mr Shipps', 'Mr Shipps'),\n",
              "              ('Mr Shipps', 'Mr Skipp'),\n",
              "              ('Mr Skipp', 'Mr Shipps'),\n",
              "              ('Mr Skipp', 'Mr Skipp'),\n",
              "              ('Mr Skipp', 'Mr Skipps'),\n",
              "              ('Mr Skipps', 'Mr Skipp'),\n",
              "              ('Mr Skipps', 'Mr Skipps'),\n",
              "              ('Mr. James Morley', 'James Ryley'),\n",
              "              ('Mr. James Morley', 'Mr James Moorley'),\n",
              "              ('Mr. James Morley', 'Mr. James Morley'),\n",
              "              ('Nassirs Nacheel', 'Nassirs Nacheel'),\n",
              "              ('Nathaniel Stackhouse', 'Nathaniel Stackhouse'),\n",
              "              ('R H Boddam', 'R H Boddam'),\n",
              "              ('Shaik Moideen', 'Hossin Mohumed'),\n",
              "              ('Shaik Moideen', 'Shaik Moideen'),\n",
              "              ('Shaik Nassir', 'Shaik Nassir'),\n",
              "              ('Shaikh Nassir', 'Shaikh Nassir'),\n",
              "              ('Thomas Hodges', 'Thomas Hodges')},\n",
              "             'QUANTITY': {('120', '120')},\n",
              "             'TITLE': {('=rine Battalion Native Infantry',\n",
              "               'Battalion Native Infantry'),\n",
              "              ('Commander', 'Commander'),\n",
              "              ('Military Paymaster', 'Military Paymaster'),\n",
              "              ('Rest._', 'Rest._'),\n",
              "              ('Seapoys', 'Seapoys'),\n",
              "              ('Seapoys', 'Sepoys'),\n",
              "              ('Sepoys', 'Seapoys'),\n",
              "              ('Sepoys', 'Sepoys'),\n",
              "              ('Subadar', 'Subadar'),\n",
              "              ('Subadar', 'Subidar')},\n",
              "             'UNKNOWN': {('Caravansera', 'Caravansera')},\n",
              "             'VESSEL': {('Betsey', 'Betsy'),\n",
              "              ('Betsey Ketch', 'Betsey Ketch'),\n",
              "              ('Betsey Ketch', 'Betsey ketch'),\n",
              "              ('Betsy', 'Betsy'),\n",
              "              ('Clive', 'Clive'),\n",
              "              ('Dolphin', 'lphin w'),\n",
              "              ('Dolphin Schooner', 'Dolphin Schooner'),\n",
              "              ('Eagle', 'Eagle'),\n",
              "              ('Essex', 'Esex'),\n",
              "              ('Essex', 'Essex'),\n",
              "              ('H.C.C. Clive', 'H.C.C. Clive'),\n",
              "              ('Marine Battn', 'Marine Battn'),\n",
              "              ('Revenge', 'Revenge'),\n",
              "              ('Revenge', 'The Revenge'),\n",
              "              ('Salamander', 'amander, a'),\n",
              "              ('Salamander', 'lamander t'),\n",
              "              ('Salamander', 'lamanders a'),\n",
              "              ('Salamanders', 'amander, a'),\n",
              "              ('Salamanders', 'lamander t'),\n",
              "              ('Salamanders', 'lamanders a'),\n",
              "              ('Schooner', 'Schooner'),\n",
              "              ('Wolfe Gallivat', 'Wolfe Gallivat')}})"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "S3S4gErg2nxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_annotations['xaa']"
      ],
      "metadata": {
        "id": "qfJ2fLz52pmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r_entities['xaa']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GMudU5p7cE-",
        "outputId": "d3262e6b-b9a9-4798-a8af-0f068efdc364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Mr. James Morley', 'PERSON '),\n",
              "  ('Bushire', 'GPE'),\n",
              "  ('Dolphin Schooner', 'VESSEL'),\n",
              "  ('Carim Caun', 'PERSON '),\n",
              "  ('Shaikhs of Boushire and Bah', 'PERSON '),\n",
              "  ('Caun', 'TITLE'),\n",
              "  ('Bushire', 'GPE'),\n",
              "  ('Ormuso', 'GPE'),\n",
              "  ('Bushire', 'GPE'),\n",
              "  ('Shaik Ice', 'PERSON '),\n",
              "  ('Bahreen', 'GPE'),\n",
              "  ('Ormuso', 'GPE'),\n",
              "  ('Dolphin', 'VESSEL'),\n",
              "  ('Ormuse', 'GPE'),\n",
              "  ('Mr Bonnyear', 'PERSON '),\n",
              "  ('Bombay', 'GPE'),\n",
              "  ('Gulph', 'LOC'),\n",
              "  ('Salamander', 'VESSEL'),\n",
              "  ('Bombay', 'GPE'),\n",
              "  ('Lieutenant Gages', 'PERSON '),\n",
              "  ('Bushire', 'GPE'),\n",
              "  ('India', 'GPE'),\n",
              "  ('Salamanders', 'VESSEL'),\n",
              "  ('Bussora', 'GPE'),\n",
              "  ('Mr Bowye', 'PERSON '),\n",
              "  ('Bombay', 'GPE'),\n",
              "  ('Mr Lyster', 'PERSON '),\n",
              "  ('Bushire', 'GPE'),\n",
              "  ('Mr. John Yoakly Botham', 'PERSON '),\n",
              "  ('Wuter', 'PERSON '),\n",
              "  ('Bushire', 'GPE'),\n",
              "  ('Shaikhs', 'TITLE'),\n",
              "  ('Schiras', 'GPE')]]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}